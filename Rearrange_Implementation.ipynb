{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMMtH2k6vs9w1jOU8l1jOT8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rokosbasilisk/einops_assignment/blob/main/Rearrange_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lark for parsing the einops expressions\n",
        "!pip install lark numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtX3GNYn8h9B",
        "outputId": "55c2a60d-2dcd-4659-e0e5-7033f7fab756"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lark in /usr/local/lib/python3.11/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from lark import Lark, Transformer"
      ],
      "metadata": {
        "id": "uXP32MVt8hMv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ELLIPSIS_TOKEN = \"<<<ELLIPSIS>>>\"\n",
        "\n",
        "einops_grammar = r\"\"\"\n",
        "    start: axes \"->\" axes\n",
        "    axes: axis*\n",
        "    axis: NAME | ONE | group | ELLIPSIS\n",
        "    group: \"(\" axes \")\"\n",
        "    ELLIPSIS: \"...\"\n",
        "    ONE: \"1\"\n",
        "    %import common.CNAME -> NAME\n",
        "    %import common.WS\n",
        "    %ignore WS\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "-OEXnJhk8wEB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatternTransformer(Transformer):\n",
        "    def start(self, items):\n",
        "        return {\"input\": items[0], \"output\": items[1]}\n",
        "\n",
        "    def axes(self, items):\n",
        "        return items\n",
        "\n",
        "    def axis(self, items):\n",
        "        return items[0]\n",
        "\n",
        "    def group(self, items):\n",
        "        # Flatten the group content into a tuple\n",
        "        if len(items) == 1 and isinstance(items[0], list):\n",
        "            return tuple(items[0])\n",
        "        return tuple(items)\n",
        "\n",
        "    def NAME(self, token):\n",
        "        return str(token)\n",
        "\n",
        "    def ONE(self, token):\n",
        "        return '1'\n",
        "\n",
        "    def ELLIPSIS(self, token):\n",
        "        return ELLIPSIS_TOKEN"
      ],
      "metadata": {
        "id": "enYtmNvS80kB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## TestCases for LARK Pattern Parser\n",
        "\n",
        "# patterns are created one for each case to be checked based on the requirements doc.\n",
        "\n",
        "patterns = [\n",
        "    (\"h w -> w h\", ['h', 'w'], ['w', 'h']),                                # Transpose\n",
        "    (\"(h w) c -> h w c\", [('h', 'w'), 'c'], ['h', 'w', 'c']),              # Split\n",
        "    (\"a b c -> (a b) c\", ['a', 'b', 'c'], [('a', 'b'), 'c']),              # Merge\n",
        "    (\"a b c -> a b 1 c\", ['a', 'b', 'c'], ['a', 'b', '1', 'c']),           # Repeat\n",
        "    (\"... h w -> ... (h w)\", ['<<<ELLIPSIS>>>', 'h', 'w'], ['<<<ELLIPSIS>>>', ('h', 'w')]) # Ellipsis\n",
        "\n",
        "]\n",
        "\n",
        "\n",
        "for pattern_str, expected_input, expected_output in patterns:\n",
        "    parsed = PatternTransformer().transform(Lark(einops_grammar).parse(pattern_str))\n",
        "    print(f\"Testing pattern: {pattern_str}\")\n",
        "    assert parsed['input'] == expected_input, f\"Input mismatch: {parsed['input']} != {expected_input}\"\n",
        "    assert parsed['output'] == expected_output, f\"Output mismatch: {parsed['output']} != {expected_output}\"\n",
        "    print(\"✓ Passed\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAz9yEo382SU",
        "outputId": "c4e0136a-b58d-464e-afeb-d649b8064fed"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing pattern: h w -> w h\n",
            "✓ Passed\n",
            "\n",
            "Testing pattern: (h w) c -> h w c\n",
            "✓ Passed\n",
            "\n",
            "Testing pattern: a b c -> (a b) c\n",
            "✓ Passed\n",
            "\n",
            "Testing pattern: a b c -> a b 1 c\n",
            "✓ Passed\n",
            "\n",
            "Testing pattern: ... h w -> ... (h w)\n",
            "✓ Passed\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EinopsRearranger:\n",
        "    def __init__(self):\n",
        "        self.parser = Lark(einops_grammar)\n",
        "\n",
        "    def rearrange(self, tensor: np.ndarray, pattern: str, **axes_lengths) -> np.ndarray:\n",
        "        print(\"\\n[REARRANGE START]\")\n",
        "        shape = list(tensor.shape)\n",
        "        print(f\"  Tensor shape: {shape}\")\n",
        "        print(f\"  Pattern: {pattern}\")\n",
        "        print(f\"  User axes_lengths: {axes_lengths}\")\n",
        "\n",
        "        # Parse\n",
        "        parsed = PatternTransformer().transform(self.parser.parse(pattern))\n",
        "        grouped_input, grouped_output = parsed['input'], parsed['output']\n",
        "        print(f\"  Grouped input: {grouped_input}\")\n",
        "        print(f\"  Grouped output: {grouped_output}\")\n",
        "\n",
        "        # Step 1: Build a basic axis_map from user lengths\n",
        "        axis_map = dict(axes_lengths)\n",
        "\n",
        "        # Step 2: Assign top-level dims, including ellipsis if any\n",
        "        self._assign_top_level_dims_ellipsis(tensor, grouped_input, axis_map)\n",
        "        print(f\"  After top-level assignment, axis_map: {axis_map}\")\n",
        "\n",
        "        # Step 3: reshape input\n",
        "        tensor = self._reshape_grouped_input(tensor, grouped_input, axis_map)\n",
        "        print(f\"  After input reshape: {tensor.shape}\")\n",
        "\n",
        "        # Step 4: transpose if needed\n",
        "        tensor = self._maybe_transpose(tensor, grouped_input, grouped_output)\n",
        "        print(f\"  After transpose: {tensor.shape}\")\n",
        "\n",
        "        # Step 5: reshape output\n",
        "        tensor = self._reshape_grouped_output(tensor, grouped_output, axis_map)\n",
        "        print(f\"  Final output shape: {tensor.shape}\")\n",
        "        return tensor\n",
        "\n",
        "    def _assign_top_level_dims_ellipsis(self, tensor, pattern, axis_map):\n",
        "        \"\"\"\n",
        "        We interpret top-level pattern axes. If we see '...', it can absorb multiple dims from 'tensor.shape'.\n",
        "        For example, '... h w' in a 4D input => '...' = first 2 dims, 'h' => 3rd dim, 'w' => 4th dim.\n",
        "        We'll store an entry axis_map['...'] = (list_of_dims).\n",
        "        We'll also handle (h w) for splits, if we can partially infer them.\n",
        "        \"\"\"\n",
        "        in_shape = list(tensor.shape)\n",
        "        shape_len = len(in_shape)\n",
        "\n",
        "        # Count how many axes are not ellipsis. We'll see how many dims are left for ellipsis.\n",
        "        top_axes_no_ellipsis = [ax for ax in pattern if ax != ELLIPSIS_TOKEN]\n",
        "\n",
        "        # We'll track shape_idx as we assign each top-level axis\n",
        "        shape_idx = 0\n",
        "\n",
        "        # figure out how many dims '...' must absorb\n",
        "        # if pattern has N top-level axes including 1 ellipsis, we do shape_len - (N-1)\n",
        "        # e.g. shape_len=4, pattern= ['...', 'h', 'w'] => '...' must absorb 2 dims\n",
        "        ellipsis_count = sum(1 for ax in pattern if ax == ELLIPSIS_TOKEN)\n",
        "        if ellipsis_count > 1:\n",
        "            raise ValueError(\"Only one ellipsis supported in this simplified approach.\")\n",
        "        leftover_for_ellipsis = 0\n",
        "        if ellipsis_count == 1:\n",
        "            leftover_for_ellipsis = shape_len - (len(pattern) - 1)\n",
        "            if leftover_for_ellipsis < 0:\n",
        "                raise ValueError(\"Not enough dims for the ellipsis in input pattern.\")\n",
        "\n",
        "        # We'll do a pass: each top-level axis either gets assigned or if it's ellipsis,\n",
        "        # it absorbs 'leftover_for_ellipsis' dims. If it's a group, we do partial inference, etc.\n",
        "        for top_axis in pattern:\n",
        "            if top_axis == ELLIPSIS_TOKEN:\n",
        "                # consume leftover_for_ellipsis dims\n",
        "                # We'll store them as axis_map['...'] = list_of_sizes\n",
        "                # or skip storing if we prefer. We'll handle it in _reshape_grouped_input.\n",
        "                axis_map['...'] = in_shape[shape_idx: shape_idx+leftover_for_ellipsis]\n",
        "                print(f\"  Ellipsis => leftover dims {axis_map['...']}\")\n",
        "                shape_idx += leftover_for_ellipsis\n",
        "\n",
        "            elif isinstance(top_axis, tuple):\n",
        "                # e.g. (h w)\n",
        "                total_dim = in_shape[shape_idx]\n",
        "                sub_names = list(top_axis)\n",
        "                known_prod = 1\n",
        "                unknowns = []\n",
        "                for nm in sub_names:\n",
        "                    if nm in axis_map:\n",
        "                        known_prod *= axis_map[nm]\n",
        "                    else:\n",
        "                        unknowns.append(nm)\n",
        "                if len(unknowns) == 1:\n",
        "                    # infer\n",
        "                    if total_dim % known_prod != 0:\n",
        "                        raise ValueError(f\"Can't split {total_dim} among group {top_axis}\")\n",
        "                    leftover = total_dim // known_prod\n",
        "                    axis_map[unknowns[0]] = leftover\n",
        "                elif len(unknowns) > 1:\n",
        "                    raise ValueError(f\"Too many unknown dims in group {top_axis}\")\n",
        "                shape_idx += 1\n",
        "            else:\n",
        "                # single name like 'c' or '1'\n",
        "                if top_axis == '1':\n",
        "                    # skip? means a new axis of size 1\n",
        "                    pass\n",
        "                elif top_axis not in axis_map:\n",
        "                    # assign from shape\n",
        "                    axis_map[top_axis] = in_shape[shape_idx]\n",
        "                shape_idx += 1\n",
        "\n",
        "    def _reshape_grouped_input(self, tensor, pattern, axis_map):\n",
        "        \"\"\"\n",
        "        If we see '...', we insert that many leftover dims from the original shape.\n",
        "        If we see (h w), we do a split, else single name => axis_map\n",
        "        \"\"\"\n",
        "        new_dims = []\n",
        "        shape_idx = 0\n",
        "\n",
        "        # We'll track the leftover '...' dims if any\n",
        "        leftover_ellipsis = axis_map.get('...', None)\n",
        "\n",
        "        for top_axis in pattern:\n",
        "            if top_axis == ELLIPSIS_TOKEN:\n",
        "                # expand leftover_ellipsis into new_dims\n",
        "                if leftover_ellipsis is None:\n",
        "                    raise ValueError(\"Ellipsis mismatch in input reshape.\")\n",
        "                print(f\"  Expanding ellipsis with leftover dims {leftover_ellipsis}\")\n",
        "                new_dims.extend(leftover_ellipsis)\n",
        "            elif isinstance(top_axis, tuple):\n",
        "                # group => splitted dimension\n",
        "                total_dim = tensor.shape[shape_idx]\n",
        "                sub_names = list(top_axis)\n",
        "                dims = []\n",
        "                known_prod = 1\n",
        "                unknowns = []\n",
        "                for s in sub_names:\n",
        "                    if s in axis_map:\n",
        "                        dims.append(axis_map[s])\n",
        "                        known_prod *= axis_map[s]\n",
        "                    else:\n",
        "                        unknowns.append(s)\n",
        "                if len(unknowns) == 1:\n",
        "                    if total_dim % known_prod != 0:\n",
        "                        raise ValueError(f\"Cannot split dimension {total_dim} for group {top_axis}\")\n",
        "                    leftover = total_dim // known_prod\n",
        "                    axis_map[unknowns[0]] = leftover\n",
        "                    dims.append(leftover)\n",
        "                elif len(unknowns) > 1:\n",
        "                    raise ValueError(f\"Too many unknowns in group {top_axis}\")\n",
        "                print(f\"  Splitting group {top_axis} => dims={dims}\")\n",
        "                new_dims.extend(dims)\n",
        "                shape_idx += 1\n",
        "            else:\n",
        "                nm = str(top_axis)\n",
        "                if nm == '1':\n",
        "                    print(\"  Found '1' -> new axis of size 1 (input).\")\n",
        "                    new_dims.append(1)\n",
        "                else:\n",
        "                    if nm not in axis_map:\n",
        "                        raise ValueError(f\"Missing axis size for '{nm}' in input reshape.\")\n",
        "                    new_dims.append(axis_map[nm])\n",
        "                shape_idx += 1\n",
        "\n",
        "        print(f\"  Input new_dims: {new_dims}\")\n",
        "        reshaped = tensor.reshape(new_dims)\n",
        "        return reshaped\n",
        "\n",
        "    def _maybe_transpose(self, tensor, grouped_input, grouped_output):\n",
        "        flat_in = []\n",
        "        for x in grouped_input:\n",
        "            if isinstance(x, tuple):\n",
        "                flat_in.extend(map(str, x))\n",
        "            elif x != ELLIPSIS_TOKEN:\n",
        "                flat_in.append(str(x))\n",
        "\n",
        "        flat_out = []\n",
        "        for x in grouped_output:\n",
        "            if isinstance(x, tuple):\n",
        "                flat_out.extend(map(str, x))\n",
        "            elif x != ELLIPSIS_TOKEN:\n",
        "                flat_out.append(str(x))\n",
        "\n",
        "        print(f\"[Transpose check]\\n  in= {flat_in}\\n  out={flat_out}\")\n",
        "        if flat_in == flat_out:\n",
        "            print(\"  No transpose needed.\")\n",
        "            return tensor\n",
        "\n",
        "        # if sets mismatch => skip\n",
        "        if sorted(flat_in) != sorted(flat_out):\n",
        "            print(\"  Mismatch in axis sets => skipping transpose.\")\n",
        "            return tensor\n",
        "\n",
        "        perm = [flat_in.index(ax) for ax in flat_out]\n",
        "        print(f\"  Applying transpose perm: {perm}\")\n",
        "        return tensor.transpose(perm)\n",
        "\n",
        "    def _reshape_grouped_output(self, tensor, pattern, axis_map):\n",
        "        \"\"\"\n",
        "        If '...', we expand leftover dims. If (h w) => merging. If '1' => new axis of size 1\n",
        "        \"\"\"\n",
        "        leftover_ellipsis = axis_map.get('...', [])\n",
        "        # we don't track shape_idx for ellipsis expansions in output\n",
        "        new_dims = []\n",
        "        shape_idx = 0\n",
        "\n",
        "        for top_axis in pattern:\n",
        "            if top_axis == ELLIPSIS_TOKEN:\n",
        "                # expand leftover\n",
        "                print(f\"  Expanding output ellipsis => {leftover_ellipsis}\")\n",
        "                new_dims.extend(leftover_ellipsis)\n",
        "            elif isinstance(top_axis, tuple):\n",
        "                # merge\n",
        "                sub_names = list(top_axis)\n",
        "                product = 1\n",
        "                for s in sub_names:\n",
        "                    if s not in axis_map:\n",
        "                        raise ValueError(f\"No axis_map for {s} in output merge.\")\n",
        "                    product *= axis_map[s]\n",
        "                print(f\"  Merging group {top_axis} => product={product}\")\n",
        "                new_dims.append(product)\n",
        "                shape_idx += 1\n",
        "            else:\n",
        "                nm = str(top_axis)\n",
        "                if nm == '1':\n",
        "                    print(\"  Found '1' => new axis of size 1 (output).\")\n",
        "                    new_dims.append(1)\n",
        "                else:\n",
        "                    if nm not in axis_map:\n",
        "                        # maybe from tensor shape\n",
        "                        if shape_idx < len(tensor.shape):\n",
        "                            axis_map[nm] = tensor.shape[shape_idx]\n",
        "                        else:\n",
        "                            axis_map[nm] = 1\n",
        "                    new_dims.append(axis_map[nm])\n",
        "                shape_idx += 1\n",
        "\n",
        "        print(f\"  Output new_dims: {new_dims}\")\n",
        "        return tensor.reshape(new_dims)"
      ],
      "metadata": {
        "id": "_s4exJ-U9CP2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test for all the five cases\n",
        "if __name__ == \"__main__\":\n",
        "    rearranger = EinopsRearranger()\n",
        "\n",
        "    # Transpose\n",
        "    x = np.random.rand(3, 4)\n",
        "    print(\"Transpose:\", rearranger.rearrange(x, 'h w -> w h').shape)\n",
        "\n",
        "    # Split\n",
        "    x = np.random.rand(12, 10)\n",
        "    print(\"Split:\", rearranger.rearrange(x, '(h w) c -> h w c', h=3).shape)\n",
        "\n",
        "    # Merge\n",
        "    x = np.random.rand(3, 4, 5)\n",
        "    print(\"Merge:\", rearranger.rearrange(x, 'a b c -> (a b) c').shape)\n",
        "\n",
        "    # Repeat\n",
        "    x = np.random.rand(3, 1, 5)\n",
        "    print(\"Repeat:\", rearranger.rearrange(x, 'a b c -> a b 1 c').shape)\n",
        "\n",
        "    # Ellipsis\n",
        "    x = np.random.rand(2, 3, 4, 5)\n",
        "    print(\"Ellipsis:\", rearranger.rearrange(x, '... h w -> ... (h w)').shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yF6-yVh39GYw",
        "outputId": "f2b2ff24-0810-4daf-d548-57b51c23dbc6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[REARRANGE START]\n",
            "  Tensor shape: [3, 4]\n",
            "  Pattern: h w -> w h\n",
            "  User axes_lengths: {}\n",
            "  Grouped input: ['h', 'w']\n",
            "  Grouped output: ['w', 'h']\n",
            "  After top-level assignment, axis_map: {'h': 3, 'w': 4}\n",
            "  Input new_dims: [3, 4]\n",
            "  After input reshape: (3, 4)\n",
            "[Transpose check]\n",
            "  in= ['h', 'w']\n",
            "  out=['w', 'h']\n",
            "  Applying transpose perm: [1, 0]\n",
            "  After transpose: (4, 3)\n",
            "  Output new_dims: [4, 3]\n",
            "  Final output shape: (4, 3)\n",
            "Transpose: (4, 3)\n",
            "\n",
            "[REARRANGE START]\n",
            "  Tensor shape: [12, 10]\n",
            "  Pattern: (h w) c -> h w c\n",
            "  User axes_lengths: {'h': 3}\n",
            "  Grouped input: [('h', 'w'), 'c']\n",
            "  Grouped output: ['h', 'w', 'c']\n",
            "  After top-level assignment, axis_map: {'h': 3, 'w': 4, 'c': 10}\n",
            "  Splitting group ('h', 'w') => dims=[3, 4]\n",
            "  Input new_dims: [3, 4, 10]\n",
            "  After input reshape: (3, 4, 10)\n",
            "[Transpose check]\n",
            "  in= ['h', 'w', 'c']\n",
            "  out=['h', 'w', 'c']\n",
            "  No transpose needed.\n",
            "  After transpose: (3, 4, 10)\n",
            "  Output new_dims: [3, 4, 10]\n",
            "  Final output shape: (3, 4, 10)\n",
            "Split: (3, 4, 10)\n",
            "\n",
            "[REARRANGE START]\n",
            "  Tensor shape: [3, 4, 5]\n",
            "  Pattern: a b c -> (a b) c\n",
            "  User axes_lengths: {}\n",
            "  Grouped input: ['a', 'b', 'c']\n",
            "  Grouped output: [('a', 'b'), 'c']\n",
            "  After top-level assignment, axis_map: {'a': 3, 'b': 4, 'c': 5}\n",
            "  Input new_dims: [3, 4, 5]\n",
            "  After input reshape: (3, 4, 5)\n",
            "[Transpose check]\n",
            "  in= ['a', 'b', 'c']\n",
            "  out=['a', 'b', 'c']\n",
            "  No transpose needed.\n",
            "  After transpose: (3, 4, 5)\n",
            "  Merging group ('a', 'b') => product=12\n",
            "  Output new_dims: [12, 5]\n",
            "  Final output shape: (12, 5)\n",
            "Merge: (12, 5)\n",
            "\n",
            "[REARRANGE START]\n",
            "  Tensor shape: [3, 1, 5]\n",
            "  Pattern: a b c -> a b 1 c\n",
            "  User axes_lengths: {}\n",
            "  Grouped input: ['a', 'b', 'c']\n",
            "  Grouped output: ['a', 'b', '1', 'c']\n",
            "  After top-level assignment, axis_map: {'a': 3, 'b': 1, 'c': 5}\n",
            "  Input new_dims: [3, 1, 5]\n",
            "  After input reshape: (3, 1, 5)\n",
            "[Transpose check]\n",
            "  in= ['a', 'b', 'c']\n",
            "  out=['a', 'b', '1', 'c']\n",
            "  Mismatch in axis sets => skipping transpose.\n",
            "  After transpose: (3, 1, 5)\n",
            "  Found '1' => new axis of size 1 (output).\n",
            "  Output new_dims: [3, 1, 1, 5]\n",
            "  Final output shape: (3, 1, 1, 5)\n",
            "Repeat: (3, 1, 1, 5)\n",
            "\n",
            "[REARRANGE START]\n",
            "  Tensor shape: [2, 3, 4, 5]\n",
            "  Pattern: ... h w -> ... (h w)\n",
            "  User axes_lengths: {}\n",
            "  Grouped input: ['<<<ELLIPSIS>>>', 'h', 'w']\n",
            "  Grouped output: ['<<<ELLIPSIS>>>', ('h', 'w')]\n",
            "  Ellipsis => leftover dims [2, 3]\n",
            "  After top-level assignment, axis_map: {'...': [2, 3], 'h': 4, 'w': 5}\n",
            "  Expanding ellipsis with leftover dims [2, 3]\n",
            "  Input new_dims: [2, 3, 4, 5]\n",
            "  After input reshape: (2, 3, 4, 5)\n",
            "[Transpose check]\n",
            "  in= ['h', 'w']\n",
            "  out=['h', 'w']\n",
            "  No transpose needed.\n",
            "  After transpose: (2, 3, 4, 5)\n",
            "  Expanding output ellipsis => [2, 3]\n",
            "  Merging group ('h', 'w') => product=20\n",
            "  Output new_dims: [2, 3, 20]\n",
            "  Final output shape: (2, 3, 20)\n",
            "Ellipsis: (2, 3, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### Testcases using Einops ###\n",
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_mkgAhT9J4S",
        "outputId": "24593839-4664-4aac-c692-5faa24c6399f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LEQOfkL8TWJ",
        "outputId": "1ef1ffb3-a149-4d82-da62-738544c91acb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[REARRANGE START]\n",
            "  Tensor shape: [3, 4]\n",
            "  Pattern: h w -> w h\n",
            "  User axes_lengths: {}\n",
            "  Grouped input: ['h', 'w']\n",
            "  Grouped output: ['w', 'h']\n",
            "  After top-level assignment, axis_map: {'h': 3, 'w': 4}\n",
            "  Input new_dims: [3, 4]\n",
            "  After input reshape: (3, 4)\n",
            "[Transpose check]\n",
            "  in= ['h', 'w']\n",
            "  out=['w', 'h']\n",
            "  Applying transpose perm: [1, 0]\n",
            "  After transpose: (4, 3)\n",
            "  Output new_dims: [4, 3]\n",
            "  Final output shape: (4, 3)\n",
            "Transpose shape (mine): (4, 3)\n",
            "Transpose shape (einops): (4, 3)\n",
            "\n",
            "[REARRANGE START]\n",
            "  Tensor shape: [12, 10]\n",
            "  Pattern: (h w) c -> h w c\n",
            "  User axes_lengths: {'h': 3}\n",
            "  Grouped input: [('h', 'w'), 'c']\n",
            "  Grouped output: ['h', 'w', 'c']\n",
            "  After top-level assignment, axis_map: {'h': 3, 'w': 4, 'c': 10}\n",
            "  Splitting group ('h', 'w') => dims=[3, 4]\n",
            "  Input new_dims: [3, 4, 10]\n",
            "  After input reshape: (3, 4, 10)\n",
            "[Transpose check]\n",
            "  in= ['h', 'w', 'c']\n",
            "  out=['h', 'w', 'c']\n",
            "  No transpose needed.\n",
            "  After transpose: (3, 4, 10)\n",
            "  Output new_dims: [3, 4, 10]\n",
            "  Final output shape: (3, 4, 10)\n",
            "Split shape (mine): (3, 4, 10)\n",
            "Split shape (einops): (3, 4, 10)\n",
            "\n",
            "[REARRANGE START]\n",
            "  Tensor shape: [3, 4, 5]\n",
            "  Pattern: a b c -> (a b) c\n",
            "  User axes_lengths: {}\n",
            "  Grouped input: ['a', 'b', 'c']\n",
            "  Grouped output: [('a', 'b'), 'c']\n",
            "  After top-level assignment, axis_map: {'a': 3, 'b': 4, 'c': 5}\n",
            "  Input new_dims: [3, 4, 5]\n",
            "  After input reshape: (3, 4, 5)\n",
            "[Transpose check]\n",
            "  in= ['a', 'b', 'c']\n",
            "  out=['a', 'b', 'c']\n",
            "  No transpose needed.\n",
            "  After transpose: (3, 4, 5)\n",
            "  Merging group ('a', 'b') => product=12\n",
            "  Output new_dims: [12, 5]\n",
            "  Final output shape: (12, 5)\n",
            "Merge shape (mine): (12, 5)\n",
            "Merge shape (einops): (12, 5)\n",
            "\n",
            "[REARRANGE START]\n",
            "  Tensor shape: [3, 1, 5]\n",
            "  Pattern: a b c -> a b 1 c\n",
            "  User axes_lengths: {}\n",
            "  Grouped input: ['a', 'b', 'c']\n",
            "  Grouped output: ['a', 'b', '1', 'c']\n",
            "  After top-level assignment, axis_map: {'a': 3, 'b': 1, 'c': 5}\n",
            "  Input new_dims: [3, 1, 5]\n",
            "  After input reshape: (3, 1, 5)\n",
            "[Transpose check]\n",
            "  in= ['a', 'b', 'c']\n",
            "  out=['a', 'b', '1', 'c']\n",
            "  Mismatch in axis sets => skipping transpose.\n",
            "  After transpose: (3, 1, 5)\n",
            "  Found '1' => new axis of size 1 (output).\n",
            "  Output new_dims: [3, 1, 1, 5]\n",
            "  Final output shape: (3, 1, 1, 5)\n",
            "Repeat shape (mine): (3, 1, 1, 5)\n",
            "Repeat shape (einops): (3, 1, 1, 5)\n",
            "\n",
            "[REARRANGE START]\n",
            "  Tensor shape: [2, 3, 4, 5]\n",
            "  Pattern: ... h w -> ... (h w)\n",
            "  User axes_lengths: {}\n",
            "  Grouped input: ['<<<ELLIPSIS>>>', 'h', 'w']\n",
            "  Grouped output: ['<<<ELLIPSIS>>>', ('h', 'w')]\n",
            "  Ellipsis => leftover dims [2, 3]\n",
            "  After top-level assignment, axis_map: {'...': [2, 3], 'h': 4, 'w': 5}\n",
            "  Expanding ellipsis with leftover dims [2, 3]\n",
            "  Input new_dims: [2, 3, 4, 5]\n",
            "  After input reshape: (2, 3, 4, 5)\n",
            "[Transpose check]\n",
            "  in= ['h', 'w']\n",
            "  out=['h', 'w']\n",
            "  No transpose needed.\n",
            "  After transpose: (2, 3, 4, 5)\n",
            "  Expanding output ellipsis => [2, 3]\n",
            "  Merging group ('h', 'w') => product=20\n",
            "  Output new_dims: [2, 3, 20]\n",
            "  Final output shape: (2, 3, 20)\n",
            "Ellipsis shape (mine): (2, 3, 20)\n",
            "Ellipsis shape (einops): (2, 3, 20)\n",
            "All tests comparing custom rearranger to einops have PASSED!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from einops import rearrange as einops_rearrange\n",
        "\n",
        "def test_transpose(einops_rearranger):\n",
        "    x = np.random.rand(3, 4)\n",
        "    my_res = einops_rearranger.rearrange(x, 'h w -> w h')\n",
        "    ref_res = einops_rearrange(x, 'h w -> w h')\n",
        "    print(\"Transpose shape (mine):\", my_res.shape)\n",
        "    print(\"Transpose shape (einops):\", ref_res.shape)\n",
        "    assert my_res.shape == ref_res.shape, \"Shape mismatch!\"\n",
        "    assert np.allclose(my_res, ref_res), \"Content mismatch!\"\n",
        "\n",
        "def test_split(einops_rearranger):\n",
        "    x = np.random.rand(12, 10)\n",
        "    my_res = einops_rearranger.rearrange(x, '(h w) c -> h w c', h=3)\n",
        "    ref_res = einops_rearrange(x, '(h w) c -> h w c', h=3)\n",
        "    print(\"Split shape (mine):\", my_res.shape)\n",
        "    print(\"Split shape (einops):\", ref_res.shape)\n",
        "    assert my_res.shape == ref_res.shape, \"Shape mismatch!\"\n",
        "    assert np.allclose(my_res, ref_res), \"Content mismatch!\"\n",
        "\n",
        "def test_merge(einops_rearranger):\n",
        "    x = np.random.rand(3, 4, 5)\n",
        "    my_res = einops_rearranger.rearrange(x, 'a b c -> (a b) c')\n",
        "    ref_res = einops_rearrange(x, 'a b c -> (a b) c')\n",
        "    print(\"Merge shape (mine):\", my_res.shape)\n",
        "    print(\"Merge shape (einops):\", ref_res.shape)\n",
        "    assert my_res.shape == ref_res.shape\n",
        "    assert np.allclose(my_res, ref_res)\n",
        "\n",
        "def test_repeat(einops_rearranger):\n",
        "    x = np.random.rand(3, 1, 5)\n",
        "    my_res = einops_rearranger.rearrange(x, 'a b c -> a b 1 c')\n",
        "    ref_res = einops_rearrange(x, 'a b c -> a b 1 c')\n",
        "    print(\"Repeat shape (mine):\", my_res.shape)\n",
        "    print(\"Repeat shape (einops):\", ref_res.shape)\n",
        "    assert my_res.shape == ref_res.shape\n",
        "    assert np.allclose(my_res, ref_res)\n",
        "\n",
        "def test_ellipsis(einops_rearranger):\n",
        "    x = np.random.rand(2, 3, 4, 5)\n",
        "    my_res = einops_rearranger.rearrange(x, '... h w -> ... (h w)')\n",
        "    ref_res = einops_rearrange(x, '... h w -> ... (h w)')\n",
        "    print(\"Ellipsis shape (mine):\", my_res.shape)\n",
        "    print(\"Ellipsis shape (einops):\", ref_res.shape)\n",
        "    assert my_res.shape == ref_res.shape\n",
        "    assert np.allclose(my_res, ref_res)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    rearranger = EinopsRearranger()\n",
        "\n",
        "    test_transpose(rearranger)\n",
        "    test_split(rearranger)\n",
        "    test_merge(rearranger)\n",
        "    test_repeat(rearranger)\n",
        "    test_ellipsis(rearranger)\n",
        "\n",
        "    print(\"All tests comparing custom rearranger to einops have PASSED!\")"
      ]
    }
  ]
}