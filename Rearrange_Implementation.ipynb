{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOW6gICF/cX6gsumkTcRBaa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rokosbasilisk/einops_assignment/blob/main/Rearrange_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lark for parsing the einops expressions\n",
        "!pip install lark numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtX3GNYn8h9B",
        "outputId": "55c2a60d-2dcd-4659-e0e5-7033f7fab756"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lark in /usr/local/lib/python3.11/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from lark import Lark, Transformer"
      ],
      "metadata": {
        "id": "uXP32MVt8hMv"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/lark-parser/lark/blob/master/docs/how_to_use.md\n",
        "\n",
        "ELLIPSIS_TOKEN = \"<<<ELLIPSIS>>>\"\n",
        "\n",
        "einops_grammar = r\"\"\"\n",
        "    start: axes \"->\" axes        # transformation from input axes to output axes\n",
        "    axes: axis*                  # a list of zero or more axes\n",
        "    axis: NAME | ONE | group | ELLIPSIS   # an axis can be a name, '1', a grouped axis, or '...'\n",
        "    group: \"(\" axes \")\"          # a group is a parenthesized list of axes\n",
        "    ELLIPSIS: \"...\"              # literal ellipsis\n",
        "    ONE: \"1\"                     # literal '1' (singleton dimension)\n",
        "    %import common.CNAME -> NAME   # import valid Python-like names as axis names\n",
        "    %import common.WS              # import whitespace\n",
        "    %ignore WS                    # ignore all whitespace in the input\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "-OEXnJhk8wEB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatternTransformer(Transformer):\n",
        "    def start(self, items):\n",
        "        return {\"input\": items[0], \"output\": items[1]}\n",
        "\n",
        "    def axes(self, items):\n",
        "        return items\n",
        "\n",
        "    def axis(self, items):\n",
        "        return items[0]\n",
        "\n",
        "    def group(self, items):\n",
        "        # Flatten the group content into a tuple\n",
        "        if len(items) == 1 and isinstance(items[0], list):\n",
        "            return tuple(items[0])\n",
        "        return tuple(items)\n",
        "\n",
        "    def NAME(self, token):\n",
        "        return str(token)\n",
        "\n",
        "    def ONE(self, token):\n",
        "        return '1'\n",
        "\n",
        "    def ELLIPSIS(self, token):\n",
        "        return ELLIPSIS_TOKEN"
      ],
      "metadata": {
        "id": "enYtmNvS80kB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## TestCases for LARK Pattern Parser\n",
        "\n",
        "# patterns are created one for each case to be checked based on the requirements doc.\n",
        "\n",
        "patterns = [\n",
        "    (\"h w -> w h\", ['h', 'w'], ['w', 'h']),                                # Transpose\n",
        "    (\"(h w) c -> h w c\", [('h', 'w'), 'c'], ['h', 'w', 'c']),              # Split\n",
        "    (\"a b c -> (a b) c\", ['a', 'b', 'c'], [('a', 'b'), 'c']),              # Merge\n",
        "    (\"a b c -> a b 1 c\", ['a', 'b', 'c'], ['a', 'b', '1', 'c']),           # Repeat\n",
        "    (\"... h w -> ... (h w)\", ['<<<ELLIPSIS>>>', 'h', 'w'], ['<<<ELLIPSIS>>>', ('h', 'w')]) # Ellipsis\n",
        "\n",
        "]\n",
        "\n",
        "\n",
        "for pattern_str, expected_input, expected_output in patterns:\n",
        "    parsed = PatternTransformer().transform(Lark(einops_grammar).parse(pattern_str))\n",
        "    print(f\"Testing pattern: {pattern_str}\")\n",
        "    assert parsed['input'] == expected_input, f\"Input mismatch: {parsed['input']} != {expected_input}\"\n",
        "    assert parsed['output'] == expected_output, f\"Output mismatch: {parsed['output']} != {expected_output}\"\n",
        "    print(\"✓ Passed\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAz9yEo382SU",
        "outputId": "f9b17f7b-4c61-4427-8fb1-f25b570fdd7f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing pattern: h w -> w h\n",
            "✓ Passed\n",
            "\n",
            "Testing pattern: (h w) c -> h w c\n",
            "✓ Passed\n",
            "\n",
            "Testing pattern: a b c -> (a b) c\n",
            "✓ Passed\n",
            "\n",
            "Testing pattern: a b c -> a b 1 c\n",
            "✓ Passed\n",
            "\n",
            "Testing pattern: ... h w -> ... (h w)\n",
            "✓ Passed\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EinopsRearranger:\n",
        "    \"\"\"\n",
        "    EinopsRearranger implements a lightweight version of einops.rearrange using:\n",
        "    - A Lark grammar parser to parse patterns like 'a b (c d) -> (a c) b d'\n",
        "    - Recursive flattening logic to handle grouped axes\n",
        "    - Axis inference from tensor shape or user-provided kwargs\n",
        "    - Ellipsis support to absorb multiple unspecified dimensions\n",
        "    - Internal debug printouts controlled via `debug` flag\n",
        "    - Pure numpy implementation, not supporting backprop stuff or torch\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, debug=False):\n",
        "        self.parser = Lark(einops_grammar)\n",
        "        self.debug = debug\n",
        "\n",
        "    def rearrange(self, tensor: np.ndarray, pattern: str, **axes_lengths) -> np.ndarray:\n",
        "        if self.debug:\n",
        "            print(\"\\n[REARRANGE START]\")\n",
        "            print(f\"  Tensor shape: {list(tensor.shape)}\")\n",
        "            print(f\"  Pattern: {pattern}\")\n",
        "            print(f\"  User axes_lengths: {axes_lengths}\")\n",
        "\n",
        "        parsed = PatternTransformer().transform(self.parser.parse(pattern))\n",
        "        grouped_input, grouped_output = parsed['input'], parsed['output']\n",
        "        if self.debug:\n",
        "            print(f\"  Grouped input: {grouped_input}\")\n",
        "            print(f\"  Grouped output: {grouped_output}\")\n",
        "\n",
        "        axis_map = dict(axes_lengths)\n",
        "        self._assign_top_level_dims_ellipsis(tensor, grouped_input, axis_map)\n",
        "        if self.debug:\n",
        "            print(f\"  After top-level assignment, axis_map: {axis_map}\")\n",
        "\n",
        "        tensor = self._reshape_grouped_input(tensor, grouped_input, axis_map)\n",
        "        if self.debug:\n",
        "            print(f\"  After input reshape: {tensor.shape}\")\n",
        "\n",
        "        tensor = self._maybe_transpose(tensor, grouped_input, grouped_output)\n",
        "        if self.debug:\n",
        "            print(f\"  After transpose: {tensor.shape}\")\n",
        "\n",
        "        tensor = self._reshape_grouped_output(tensor, grouped_output, axis_map)\n",
        "        if self.debug:\n",
        "            print(f\"  Final output shape: {tensor.shape}\")\n",
        "        return tensor\n",
        "\n",
        "    def _assign_top_level_dims_ellipsis(self, tensor, pattern, axis_map):\n",
        "        in_shape = list(tensor.shape)\n",
        "        shape_len = len(in_shape)\n",
        "        top_axes_no_ellipsis = [ax for ax in pattern if ax != ELLIPSIS_TOKEN]\n",
        "        shape_idx = 0\n",
        "        ellipsis_count = sum(1 for ax in pattern if ax == ELLIPSIS_TOKEN)\n",
        "\n",
        "        if ellipsis_count > 1:\n",
        "            raise ValueError(\"Only one ellipsis supported.\")\n",
        "        leftover_for_ellipsis = shape_len - (len(pattern) - 1) if ellipsis_count else 0\n",
        "        if leftover_for_ellipsis < 0:\n",
        "            raise ValueError(\"Not enough dims for ellipsis.\")\n",
        "\n",
        "        for top_axis in pattern:\n",
        "            if top_axis == ELLIPSIS_TOKEN:\n",
        "                axis_map['...'] = in_shape[shape_idx: shape_idx + leftover_for_ellipsis]\n",
        "                if self.debug:\n",
        "                    print(f\"  Ellipsis => leftover dims {axis_map['...']}\")\n",
        "                shape_idx += leftover_for_ellipsis\n",
        "            elif isinstance(top_axis, tuple):\n",
        "                total_dim = in_shape[shape_idx]\n",
        "                sub_names = list(top_axis)\n",
        "                known_prod = 1\n",
        "                unknowns = []\n",
        "                for nm in sub_names:\n",
        "                    if nm in axis_map:\n",
        "                        known_prod *= axis_map[nm]\n",
        "                    else:\n",
        "                        unknowns.append(nm)\n",
        "                if len(unknowns) == 1:\n",
        "                    if total_dim % known_prod != 0:\n",
        "                        raise ValueError(f\"Can't split {total_dim} among group {top_axis}\")\n",
        "                    axis_map[unknowns[0]] = total_dim // known_prod\n",
        "                elif len(unknowns) > 1:\n",
        "                    raise ValueError(f\"Too many unknown dims in group {top_axis}\")\n",
        "                shape_idx += 1\n",
        "            else:\n",
        "                if top_axis != '1' and top_axis not in axis_map:\n",
        "                    axis_map[top_axis] = in_shape[shape_idx]\n",
        "                shape_idx += 1\n",
        "\n",
        "    def _reshape_grouped_input(self, tensor, pattern, axis_map):\n",
        "        new_dims = []\n",
        "        shape_idx = 0\n",
        "        leftover_ellipsis = axis_map.get('...', None)\n",
        "\n",
        "        for top_axis in pattern:\n",
        "            if top_axis == ELLIPSIS_TOKEN:\n",
        "                if leftover_ellipsis is None:\n",
        "                    raise ValueError(\"Ellipsis mismatch in input reshape.\")\n",
        "                if self.debug:\n",
        "                    print(f\"  Expanding ellipsis with leftover dims {leftover_ellipsis}\")\n",
        "                new_dims.extend(leftover_ellipsis)\n",
        "            elif isinstance(top_axis, tuple):\n",
        "                total_dim = tensor.shape[shape_idx]\n",
        "                dims = []\n",
        "                known_prod = 1\n",
        "                unknowns = []\n",
        "                for s in top_axis:\n",
        "                    if s in axis_map:\n",
        "                        dims.append(axis_map[s])\n",
        "                        known_prod *= axis_map[s]\n",
        "                    else:\n",
        "                        unknowns.append(s)\n",
        "                if len(unknowns) == 1:\n",
        "                    if total_dim % known_prod != 0:\n",
        "                        raise ValueError(f\"Cannot split dimension {total_dim} for group {top_axis}\")\n",
        "                    leftover = total_dim // known_prod\n",
        "                    axis_map[unknowns[0]] = leftover\n",
        "                    dims.append(leftover)\n",
        "                elif len(unknowns) > 1:\n",
        "                    raise ValueError(f\"Too many unknowns in group {top_axis}\")\n",
        "                if self.debug:\n",
        "                    print(f\"  Splitting group {top_axis} => dims={dims}\")\n",
        "                new_dims.extend(dims)\n",
        "                shape_idx += 1\n",
        "            else:\n",
        "                nm = str(top_axis)\n",
        "                if nm == '1':\n",
        "                    if self.debug:\n",
        "                        print(\"  Found '1' -> new axis of size 1 (input).\")\n",
        "                    new_dims.append(1)\n",
        "                else:\n",
        "                    new_dims.append(axis_map[nm])\n",
        "                shape_idx += 1\n",
        "\n",
        "        if self.debug:\n",
        "            print(f\"  Input new_dims: {new_dims}\")\n",
        "        return tensor.reshape(new_dims)\n",
        "\n",
        "    def _maybe_transpose(self, tensor, grouped_input, grouped_output):\n",
        "        flat_in = []\n",
        "        for x in grouped_input:\n",
        "            if isinstance(x, tuple):\n",
        "                flat_in.extend(map(str, x))\n",
        "            elif x != ELLIPSIS_TOKEN:\n",
        "                flat_in.append(str(x))\n",
        "\n",
        "        flat_out = []\n",
        "        for x in grouped_output:\n",
        "            if isinstance(x, tuple):\n",
        "                flat_out.extend(map(str, x))\n",
        "            elif x != ELLIPSIS_TOKEN:\n",
        "                flat_out.append(str(x))\n",
        "\n",
        "        if self.debug:\n",
        "            print(f\"[Transpose check]\\n  in= {flat_in}\\n  out={flat_out}\")\n",
        "        if flat_in == flat_out:\n",
        "            if self.debug:\n",
        "                print(\"  No transpose needed.\")\n",
        "            return tensor\n",
        "\n",
        "        if sorted(flat_in) != sorted(flat_out):\n",
        "            if self.debug:\n",
        "                print(\"  Mismatch in axis sets => skipping transpose.\")\n",
        "            return tensor\n",
        "\n",
        "        perm = [flat_in.index(ax) for ax in flat_out]\n",
        "        if self.debug:\n",
        "            print(f\"  Applying transpose perm: {perm}\")\n",
        "        return tensor.transpose(perm)\n",
        "\n",
        "    def _reshape_grouped_output(self, tensor, pattern, axis_map):\n",
        "        leftover_ellipsis = axis_map.get('...', [])\n",
        "        new_dims = []\n",
        "        shape_idx = 0\n",
        "\n",
        "        for top_axis in pattern:\n",
        "            if top_axis == ELLIPSIS_TOKEN:\n",
        "                if self.debug:\n",
        "                    print(f\"  Expanding output ellipsis => {leftover_ellipsis}\")\n",
        "                new_dims.extend(leftover_ellipsis)\n",
        "            elif isinstance(top_axis, tuple):\n",
        "                product = 1\n",
        "                for s in top_axis:\n",
        "                    product *= axis_map[s]\n",
        "                if self.debug:\n",
        "                    print(f\"  Merging group {top_axis} => product={product}\")\n",
        "                new_dims.append(product)\n",
        "                shape_idx += 1\n",
        "            else:\n",
        "                nm = str(top_axis)\n",
        "                if nm == '1':\n",
        "                    if self.debug:\n",
        "                        print(\"  Found '1' => new axis of size 1 (output).\")\n",
        "                    new_dims.append(1)\n",
        "                else:\n",
        "                    if nm not in axis_map:\n",
        "                        if shape_idx < len(tensor.shape):\n",
        "                            axis_map[nm] = tensor.shape[shape_idx]\n",
        "                        else:\n",
        "                            axis_map[nm] = 1\n",
        "                    new_dims.append(axis_map[nm])\n",
        "                shape_idx += 1\n",
        "\n",
        "        if self.debug:\n",
        "            print(f\"  Output new_dims: {new_dims}\")\n",
        "        return tensor.reshape(new_dims)\n"
      ],
      "metadata": {
        "id": "_s4exJ-U9CP2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test for all the five cases mentioned in the assignment document"
      ],
      "metadata": {
        "id": "dlJdp8ZbDTMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    rearranger = EinopsRearranger(debug=True)\n",
        "\n",
        "    # Transpose\n",
        "    x = np.random.rand(3, 4)\n",
        "    print(\"Transpose:\", rearranger.rearrange(x, 'h w -> w h').shape)\n",
        "\n",
        "    # Split\n",
        "    x = np.random.rand(12, 10)\n",
        "    print(\"Split:\", rearranger.rearrange(x, '(h w) c -> h w c', h=3).shape)\n",
        "\n",
        "    # Merge\n",
        "    x = np.random.rand(3, 4, 5)\n",
        "    print(\"Merge:\", rearranger.rearrange(x, 'a b c -> (a b) c').shape)\n",
        "\n",
        "    # Repeat\n",
        "    x = np.random.rand(3, 1, 5)\n",
        "    print(\"Repeat:\", rearranger.rearrange(x, 'a b c -> a b 1 c').shape)\n",
        "\n",
        "    # Ellipsis\n",
        "    x = np.random.rand(2, 3, 4, 5)\n",
        "    print(\"Ellipsis:\", rearranger.rearrange(x, '... h w -> ... (h w)').shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yF6-yVh39GYw",
        "outputId": "5165c89b-d788-4fb7-bb3e-d94c9917cc54"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[REARRANGE START]\n",
            "  Tensor shape: [3, 4]\n",
            "  Pattern: h w -> w h\n",
            "  User axes_lengths: {}\n",
            "  Grouped input: ['h', 'w']\n",
            "  Grouped output: ['w', 'h']\n",
            "  After top-level assignment, axis_map: {'h': 3, 'w': 4}\n",
            "  Input new_dims: [3, 4]\n",
            "  After input reshape: (3, 4)\n",
            "[Transpose check]\n",
            "  in= ['h', 'w']\n",
            "  out=['w', 'h']\n",
            "  Applying transpose perm: [1, 0]\n",
            "  After transpose: (4, 3)\n",
            "  Output new_dims: [4, 3]\n",
            "  Final output shape: (4, 3)\n",
            "Transpose: (4, 3)\n",
            "\n",
            "[REARRANGE START]\n",
            "  Tensor shape: [12, 10]\n",
            "  Pattern: (h w) c -> h w c\n",
            "  User axes_lengths: {'h': 3}\n",
            "  Grouped input: [('h', 'w'), 'c']\n",
            "  Grouped output: ['h', 'w', 'c']\n",
            "  After top-level assignment, axis_map: {'h': 3, 'w': 4, 'c': 10}\n",
            "  Splitting group ('h', 'w') => dims=[3, 4]\n",
            "  Input new_dims: [3, 4, 10]\n",
            "  After input reshape: (3, 4, 10)\n",
            "[Transpose check]\n",
            "  in= ['h', 'w', 'c']\n",
            "  out=['h', 'w', 'c']\n",
            "  No transpose needed.\n",
            "  After transpose: (3, 4, 10)\n",
            "  Output new_dims: [3, 4, 10]\n",
            "  Final output shape: (3, 4, 10)\n",
            "Split: (3, 4, 10)\n",
            "\n",
            "[REARRANGE START]\n",
            "  Tensor shape: [3, 4, 5]\n",
            "  Pattern: a b c -> (a b) c\n",
            "  User axes_lengths: {}\n",
            "  Grouped input: ['a', 'b', 'c']\n",
            "  Grouped output: [('a', 'b'), 'c']\n",
            "  After top-level assignment, axis_map: {'a': 3, 'b': 4, 'c': 5}\n",
            "  Input new_dims: [3, 4, 5]\n",
            "  After input reshape: (3, 4, 5)\n",
            "[Transpose check]\n",
            "  in= ['a', 'b', 'c']\n",
            "  out=['a', 'b', 'c']\n",
            "  No transpose needed.\n",
            "  After transpose: (3, 4, 5)\n",
            "  Merging group ('a', 'b') => product=12\n",
            "  Output new_dims: [12, 5]\n",
            "  Final output shape: (12, 5)\n",
            "Merge: (12, 5)\n",
            "\n",
            "[REARRANGE START]\n",
            "  Tensor shape: [3, 1, 5]\n",
            "  Pattern: a b c -> a b 1 c\n",
            "  User axes_lengths: {}\n",
            "  Grouped input: ['a', 'b', 'c']\n",
            "  Grouped output: ['a', 'b', '1', 'c']\n",
            "  After top-level assignment, axis_map: {'a': 3, 'b': 1, 'c': 5}\n",
            "  Input new_dims: [3, 1, 5]\n",
            "  After input reshape: (3, 1, 5)\n",
            "[Transpose check]\n",
            "  in= ['a', 'b', 'c']\n",
            "  out=['a', 'b', '1', 'c']\n",
            "  Mismatch in axis sets => skipping transpose.\n",
            "  After transpose: (3, 1, 5)\n",
            "  Found '1' => new axis of size 1 (output).\n",
            "  Output new_dims: [3, 1, 1, 5]\n",
            "  Final output shape: (3, 1, 1, 5)\n",
            "Repeat: (3, 1, 1, 5)\n",
            "\n",
            "[REARRANGE START]\n",
            "  Tensor shape: [2, 3, 4, 5]\n",
            "  Pattern: ... h w -> ... (h w)\n",
            "  User axes_lengths: {}\n",
            "  Grouped input: ['<<<ELLIPSIS>>>', 'h', 'w']\n",
            "  Grouped output: ['<<<ELLIPSIS>>>', ('h', 'w')]\n",
            "  Ellipsis => leftover dims [2, 3]\n",
            "  After top-level assignment, axis_map: {'...': [2, 3], 'h': 4, 'w': 5}\n",
            "  Expanding ellipsis with leftover dims [2, 3]\n",
            "  Input new_dims: [2, 3, 4, 5]\n",
            "  After input reshape: (2, 3, 4, 5)\n",
            "[Transpose check]\n",
            "  in= ['h', 'w']\n",
            "  out=['h', 'w']\n",
            "  No transpose needed.\n",
            "  After transpose: (2, 3, 4, 5)\n",
            "  Expanding output ellipsis => [2, 3]\n",
            "  Merging group ('h', 'w') => product=20\n",
            "  Output new_dims: [2, 3, 20]\n",
            "  Final output shape: (2, 3, 20)\n",
            "Ellipsis: (2, 3, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testcases using Einops ###"
      ],
      "metadata": {
        "id": "vwBqskJMDL54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_mkgAhT9J4S",
        "outputId": "fda39a88-6c17-47b9-e370-97ffb50e1041"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LEQOfkL8TWJ",
        "outputId": "7e44d327-2c55-4e2a-9b5c-8469f5167c6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transpose shape (mine): (4, 3)\n",
            "Transpose shape (einops): (4, 3)\n",
            "Split shape (mine): (3, 4, 10)\n",
            "Split shape (einops): (3, 4, 10)\n",
            "Merge shape (mine): (12, 5)\n",
            "Merge shape (einops): (12, 5)\n",
            "Repeat shape (mine): (3, 1, 1, 5)\n",
            "Repeat shape (einops): (3, 1, 1, 5)\n",
            "Ellipsis shape (mine): (2, 3, 20)\n",
            "Ellipsis shape (einops): (2, 3, 20)\n",
            "All tests comparing custom rearranger to einops have PASSED!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from einops import rearrange as einops_rearrange\n",
        "\n",
        "def test_transpose(einops_rearranger):\n",
        "    x = np.random.rand(3, 4)\n",
        "    my_res = einops_rearranger.rearrange(x, 'h w -> w h')\n",
        "    ref_res = einops_rearrange(x, 'h w -> w h')\n",
        "    print(\"Transpose shape (mine):\", my_res.shape)\n",
        "    print(\"Transpose shape (einops):\", ref_res.shape)\n",
        "    assert my_res.shape == ref_res.shape, \"Shape mismatch!\"\n",
        "    assert np.allclose(my_res, ref_res), \"Content mismatch!\"\n",
        "\n",
        "def test_split(einops_rearranger):\n",
        "    x = np.random.rand(12, 10)\n",
        "    my_res = einops_rearranger.rearrange(x, '(h w) c -> h w c', h=3)\n",
        "    ref_res = einops_rearrange(x, '(h w) c -> h w c', h=3)\n",
        "    print(\"Split shape (mine):\", my_res.shape)\n",
        "    print(\"Split shape (einops):\", ref_res.shape)\n",
        "    assert my_res.shape == ref_res.shape, \"Shape mismatch!\"\n",
        "    assert np.allclose(my_res, ref_res), \"Content mismatch!\"\n",
        "\n",
        "def test_merge(einops_rearranger):\n",
        "    x = np.random.rand(3, 4, 5)\n",
        "    my_res = einops_rearranger.rearrange(x, 'a b c -> (a b) c')\n",
        "    ref_res = einops_rearrange(x, 'a b c -> (a b) c')\n",
        "    print(\"Merge shape (mine):\", my_res.shape)\n",
        "    print(\"Merge shape (einops):\", ref_res.shape)\n",
        "    assert my_res.shape == ref_res.shape\n",
        "    assert np.allclose(my_res, ref_res)\n",
        "\n",
        "def test_repeat(einops_rearranger):\n",
        "    x = np.random.rand(3, 1, 5)\n",
        "    my_res = einops_rearranger.rearrange(x, 'a b c -> a b 1 c')\n",
        "    ref_res = einops_rearrange(x, 'a b c -> a b 1 c')\n",
        "    print(\"Repeat shape (mine):\", my_res.shape)\n",
        "    print(\"Repeat shape (einops):\", ref_res.shape)\n",
        "    assert my_res.shape == ref_res.shape\n",
        "    assert np.allclose(my_res, ref_res)\n",
        "\n",
        "def test_ellipsis(einops_rearranger):\n",
        "    x = np.random.rand(2, 3, 4, 5)\n",
        "    my_res = einops_rearranger.rearrange(x, '... h w -> ... (h w)')\n",
        "    ref_res = einops_rearrange(x, '... h w -> ... (h w)')\n",
        "    print(\"Ellipsis shape (mine):\", my_res.shape)\n",
        "    print(\"Ellipsis shape (einops):\", ref_res.shape)\n",
        "    assert my_res.shape == ref_res.shape\n",
        "    assert np.allclose(my_res, ref_res)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    rearranger = EinopsRearranger()\n",
        "\n",
        "    test_transpose(rearranger)\n",
        "    test_split(rearranger)\n",
        "    test_merge(rearranger)\n",
        "    test_repeat(rearranger)\n",
        "    test_ellipsis(rearranger)\n",
        "\n",
        "    print(\"All tests comparing custom rearranger to einops have PASSED!\")"
      ]
    }
  ]
}